{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练深度神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization批量标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (reset_tf_graph, show_tf_graph)\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载mnist数据\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/envs/dev/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "reset_tf_graph()\n",
    "\n",
    "n_inputs = 28*28 # minst里的图像分辨率是28*28\n",
    "n_hidden1 = 300 # 第一个隐藏层的神经元数量\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10 # 分类10个数字\n",
    "\n",
    "batch_norm_momentum = 0.9 # 标准化的偏移量\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.variance_scaling_initializer() # 使用HE初始化权重\n",
    "\n",
    "    my_batch_norm_layer = partial( # 类似做函数的柯里化curring, 填入部分函数值生成新的函数\n",
    "            tf.layers.batch_normalization,\n",
    "            training=training,\n",
    "            momentum=batch_norm_momentum)\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "            tf.layers.dense,\n",
    "            kernel_initializer=he_init)\n",
    "\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1)) # 标准化后再执行激活函数ELU - 指数线性单元\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name=\"hidden2\")\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logits_before_bn = my_dense_layer(bn2, n_outputs, name=\"outputs\")\n",
    "    logits = my_batch_norm_layer(logits_before_bn)\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.8952\n",
      "1 Validation accuracy: 0.9202\n",
      "2 Validation accuracy: 0.9318\n",
      "3 Validation accuracy: 0.9422\n",
      "4 Validation accuracy: 0.9468\n",
      "5 Validation accuracy: 0.954\n",
      "6 Validation accuracy: 0.9568\n",
      "7 Validation accuracy: 0.96\n",
      "8 Validation accuracy: 0.962\n",
      "9 Validation accuracy: 0.9638\n",
      "10 Validation accuracy: 0.9662\n",
      "11 Validation accuracy: 0.9682\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9696\n",
      "14 Validation accuracy: 0.9706\n",
      "15 Validation accuracy: 0.9704\n",
      "16 Validation accuracy: 0.9718\n",
      "17 Validation accuracy: 0.9726\n",
      "18 Validation accuracy: 0.9738\n",
      "19 Validation accuracy: 0.9742\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "# 定义一个从训练集中随机挑选50个实例的方法\n",
    "def batch_generator(X, y, size):\n",
    "    rnd_idx = np.random.permutation(len(X)) # len(X)是矩阵X的第0维的长度, 生成0..len(X)的随机数\n",
    "    n_batches = len(X) // batch_size # //是整除, mod\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch # yield定义了生成器generator, 可以用next(generator)来调用, 也可以用for循环调用\n",
    "\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, batch_size):\n",
    "            sess.run([training_op, extra_update_ops],\n",
    "                     feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./chpt11/my_model_final.ckpt\")\n",
    "    \n",
    "file_writer = tf.summary.FileWriter(\"./chpt11/graph\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hidden1/kernel:0',\n",
       " 'hidden1/bias:0',\n",
       " 'batch_normalization/gamma:0',\n",
       " 'batch_normalization/beta:0',\n",
       " 'batch_normalization/moving_mean:0',\n",
       " 'batch_normalization/moving_variance:0',\n",
       " 'hidden2/kernel:0',\n",
       " 'hidden2/bias:0',\n",
       " 'batch_normalization_1/gamma:0',\n",
       " 'batch_normalization_1/beta:0',\n",
       " 'batch_normalization_1/moving_mean:0',\n",
       " 'batch_normalization_1/moving_variance:0',\n",
       " 'outputs/kernel:0',\n",
       " 'outputs/bias:0',\n",
       " 'batch_normalization_2/gamma:0',\n",
       " 'batch_normalization_2/beta:0',\n",
       " 'batch_normalization_2/moving_mean:0',\n",
       " 'batch_normalization_2/moving_variance:0']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.name for v in tf.global_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAX Norm 最大范数约束正则化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (reset_tf_graph, show_tf_graph)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 加载mnist数据\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm\n",
    "\n",
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\", kernel_regularizer=max_norm_reg)\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\", kernel_regularizer=max_norm_reg)\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义权重裁剪操作\n",
    "\n",
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)\n",
    "\n",
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9562\n",
      "1 Validation accuracy: 0.9702\n",
      "2 Validation accuracy: 0.9728\n",
      "3 Validation accuracy: 0.9754\n",
      "4 Validation accuracy: 0.9748\n",
      "5 Validation accuracy: 0.9776\n",
      "6 Validation accuracy: 0.9798\n",
      "7 Validation accuracy: 0.9814\n",
      "8 Validation accuracy: 0.9816\n",
      "9 Validation accuracy: 0.9818\n",
      "10 Validation accuracy: 0.982\n",
      "11 Validation accuracy: 0.9802\n",
      "12 Validation accuracy: 0.9808\n",
      "13 Validation accuracy: 0.9808\n",
      "14 Validation accuracy: 0.982\n",
      "15 Validation accuracy: 0.9814\n",
      "16 Validation accuracy: 0.9814\n",
      "17 Validation accuracy: 0.983\n",
      "18 Validation accuracy: 0.982\n",
      "19 Validation accuracy: 0.9822\n"
     ]
    }
   ],
   "source": [
    "# 定义一个从训练集中随机挑选50个实例的方法\n",
    "def batch_generator(X, y, size):\n",
    "    rnd_idx = np.random.permutation(len(X)) # len(X)是矩阵X的第0维的长度, 生成0..len(X)的随机数\n",
    "    n_batches = len(X) // batch_size # //是整除, mod\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch # yield定义了生成器generator, 可以用next(generator)来调用, 也可以用for循环调用\n",
    "\n",
    "with tf.Session() as sess:                                              \n",
    "    init.run()                                                          \n",
    "    for epoch in range(n_epochs):                                       \n",
    "        for X_batch, y_batch in batch_generator(X_train, y_train, batch_size): \n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        \n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   \n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 \n",
    "\n",
    "    save_path = saver.save(sess, \"./chpt11/model/max_norm.ckpt\")\n",
    "    \n",
    "file_writer = tf.summary.FileWriter(\"./chpt11/graph/max_norm\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 习题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.深度学习。\n",
    "\n",
    "i.  建立一个 DNN,有五个隐藏层,每层 100 个神经元,使用 He 初始化和 ELU 激活函数。\n",
    "\n",
    "ii.  使用 Adam 优化和提前停止,请尝试在 MNIST 上进行训练,但只能使用数字 0 到4,因为我们将在下一个练习中在数字 5 到 9 上进行迁移学习。 您需要一个包含五个神经元的 softmax 输出层,并且一如既往地确保定期保存检查点,并保存最终模型,以便稍后再使用它。\n",
    "\n",
    "iii.  使用交叉验证调整超参数,并查看你能达到什么准确度。\n",
    "\n",
    "iv.  现在尝试添加批量标准化并比较学习曲线:它是否比以前收敛得更快? 它是否会产生更好的模型? v.  模型是否过拟合训练集? 尝试将 dropout 添加到每一层,然后重试。 它有帮助吗?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (reset_tf_graph, show_tf_graph)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载mnist数据\n",
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取数据集中0~4的部分\n",
    "X_train_04 = X_train[y_train <= 4]\n",
    "y_train_04 = y_train[y_train <= 4]\n",
    "X_valid_04 = X_valid[y_valid <= 4]\n",
    "y_valid_04 = y_valid[y_valid <= 4]\n",
    "X_test_04 = X_test[y_test <= 4]\n",
    "y_test_04 = y_test[y_test <= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976a8048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976a8048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976a8048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976a8048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896b217b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896b217b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896b217b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896b217b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa89bb372b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa89bb372b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa89bb372b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa89bb372b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976311d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976311d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976311d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa8976311d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa897631518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa897631518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa897631518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa897631518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896d6d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896d6d550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896d6d550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fa896d6d550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "n_inputs = 28*28\n",
    "n_outputs = 5\n",
    "n_hidden_neurons = 500\n",
    "n_hidden_layers = 5\n",
    "\n",
    "reset_tf_graph()\n",
    "\n",
    "# 定义输入参数\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "# 定义层\n",
    "he_init = tf.initializers.variance_scaling()\n",
    "my_hidden_layer = partial(tf.layers.dense, \n",
    "                          activation=tf.nn.elu,\n",
    "                          kernel_initializer=he_init)\n",
    "\n",
    "def make_dnn(inputs):\n",
    "    with tf.name_scope(\"dnn\") as scope:\n",
    "        for n in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_hidden_neurons, \n",
    "                                     activation=tf.nn.elu, kernel_initializer=he_init,\n",
    "                                     name=\"hidden%d\" % (n + 1))\n",
    "        return inputs\n",
    "\n",
    "dnn_outputs = make_dnn(X)\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, \n",
    "                         kernel_initializer=he_init,\n",
    "                        name=\"logits\")\n",
    "y_probs = tf.nn.softmax(logits, name=\"y_probs\")\n",
    "\n",
    "# 定义交叉熵损失函数\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# 定义训练操作\n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  0.1368396\n",
      "0 Validation accuracy: 0.9589523\n",
      "epoch: 0, validation loss: 0.136840, best loss: 0.136840, accuracy: 95.90%\n",
      "1 loss:  0.09745088\n",
      "1 Validation accuracy: 0.9702893\n",
      "epoch: 1, validation loss: 0.097451, best loss: 0.097451, accuracy: 97.03%\n",
      "2 loss:  0.09227297\n",
      "2 Validation accuracy: 0.9745895\n",
      "epoch: 2, validation loss: 0.092273, best loss: 0.092273, accuracy: 97.46%\n",
      "3 loss:  0.096482694\n",
      "3 Validation accuracy: 0.97263485\n",
      "epoch: 3, validation loss: 0.096483, best loss: 0.092273, accuracy: 97.26%\n",
      "4 loss:  0.075700276\n",
      "4 Validation accuracy: 0.97888976\n",
      "epoch: 4, validation loss: 0.075700, best loss: 0.075700, accuracy: 97.89%\n",
      "5 loss:  0.07595821\n",
      "5 Validation accuracy: 0.9792807\n",
      "epoch: 5, validation loss: 0.075958, best loss: 0.075700, accuracy: 97.93%\n",
      "6 loss:  0.06880345\n",
      "6 Validation accuracy: 0.9816263\n",
      "epoch: 6, validation loss: 0.068803, best loss: 0.068803, accuracy: 98.16%\n",
      "7 loss:  0.06192577\n",
      "7 Validation accuracy: 0.98319\n",
      "epoch: 7, validation loss: 0.061926, best loss: 0.061926, accuracy: 98.32%\n",
      "8 loss:  0.046409465\n",
      "8 Validation accuracy: 0.98670834\n",
      "epoch: 8, validation loss: 0.046409, best loss: 0.046409, accuracy: 98.67%\n",
      "9 loss:  0.05505774\n",
      "9 Validation accuracy: 0.9859265\n",
      "epoch: 9, validation loss: 0.055058, best loss: 0.046409, accuracy: 98.59%\n",
      "10 loss:  0.05869168\n",
      "10 Validation accuracy: 0.9843628\n",
      "epoch: 10, validation loss: 0.058692, best loss: 0.046409, accuracy: 98.44%\n",
      "11 loss:  0.052690998\n",
      "11 Validation accuracy: 0.98670834\n",
      "epoch: 11, validation loss: 0.052691, best loss: 0.046409, accuracy: 98.67%\n",
      "12 loss:  0.07024001\n",
      "12 Validation accuracy: 0.97888976\n",
      "epoch: 12, validation loss: 0.070240, best loss: 0.046409, accuracy: 97.89%\n",
      "13 loss:  0.06317537\n",
      "13 Validation accuracy: 0.9816263\n",
      "epoch: 13, validation loss: 0.063175, best loss: 0.046409, accuracy: 98.16%\n",
      "14 loss:  0.040347278\n",
      "14 Validation accuracy: 0.99022675\n",
      "epoch: 14, validation loss: 0.040347, best loss: 0.040347, accuracy: 99.02%\n",
      "15 loss:  0.05693875\n",
      "15 Validation accuracy: 0.98553556\n",
      "epoch: 15, validation loss: 0.056939, best loss: 0.040347, accuracy: 98.55%\n",
      "16 loss:  0.04872443\n",
      "16 Validation accuracy: 0.98749024\n",
      "epoch: 16, validation loss: 0.048724, best loss: 0.040347, accuracy: 98.75%\n",
      "17 loss:  0.08298838\n",
      "17 Validation accuracy: 0.9820172\n",
      "epoch: 17, validation loss: 0.082988, best loss: 0.040347, accuracy: 98.20%\n",
      "18 loss:  0.035105005\n",
      "18 Validation accuracy: 0.9925723\n",
      "epoch: 18, validation loss: 0.035105, best loss: 0.035105, accuracy: 99.26%\n",
      "19 loss:  4.8556232\n",
      "19 Validation accuracy: 0.20093824\n",
      "epoch: 19, validation loss: 4.855623, best loss: 0.035105, accuracy: 20.09%\n",
      "20 loss:  1.6908191\n",
      "20 Validation accuracy: 0.20914777\n",
      "epoch: 20, validation loss: 1.690819, best loss: 0.035105, accuracy: 20.91%\n",
      "21 loss:  1.6684685\n",
      "21 Validation accuracy: 0.20914777\n",
      "epoch: 21, validation loss: 1.668468, best loss: 0.035105, accuracy: 20.91%\n",
      "22 loss:  1.6461189\n",
      "22 Validation accuracy: 0.22009382\n",
      "epoch: 22, validation loss: 1.646119, best loss: 0.035105, accuracy: 22.01%\n",
      "23 loss:  1.6623446\n",
      "23 Validation accuracy: 0.1927287\n",
      "epoch: 23, validation loss: 1.662345, best loss: 0.035105, accuracy: 19.27%\n",
      "24 loss:  1.6317708\n",
      "24 Validation accuracy: 0.20914777\n",
      "epoch: 24, validation loss: 1.631771, best loss: 0.035105, accuracy: 20.91%\n",
      "25 loss:  1.6477927\n",
      "25 Validation accuracy: 0.18725567\n",
      "epoch: 25, validation loss: 1.647793, best loss: 0.035105, accuracy: 18.73%\n",
      "26 loss:  1.6567957\n",
      "26 Validation accuracy: 0.18725567\n",
      "epoch: 26, validation loss: 1.656796, best loss: 0.035105, accuracy: 18.73%\n",
      "27 loss:  1.8166231\n",
      "27 Validation accuracy: 0.1927287\n",
      "epoch: 27, validation loss: 1.816623, best loss: 0.035105, accuracy: 19.27%\n",
      "28 loss:  1.6375792\n",
      "28 Validation accuracy: 0.22009382\n",
      "epoch: 28, validation loss: 1.637579, best loss: 0.035105, accuracy: 22.01%\n",
      "29 loss:  1.6545153\n",
      "29 Validation accuracy: 0.20914777\n",
      "epoch: 29, validation loss: 1.654515, best loss: 0.035105, accuracy: 20.91%\n",
      "30 loss:  1.6905348\n",
      "30 Validation accuracy: 0.18725567\n",
      "epoch: 30, validation loss: 1.690535, best loss: 0.035105, accuracy: 18.73%\n",
      "31 loss:  1.6612097\n",
      "31 Validation accuracy: 0.22009382\n",
      "epoch: 31, validation loss: 1.661210, best loss: 0.035105, accuracy: 22.01%\n",
      "32 loss:  1.6639435\n",
      "32 Validation accuracy: 0.18725567\n",
      "epoch: 32, validation loss: 1.663944, best loss: 0.035105, accuracy: 18.73%\n",
      "33 loss:  1.6852863\n",
      "33 Validation accuracy: 0.19077404\n",
      "epoch: 33, validation loss: 1.685286, best loss: 0.035105, accuracy: 19.08%\n",
      "34 loss:  1.7823476\n",
      "34 Validation accuracy: 0.19077404\n",
      "epoch: 34, validation loss: 1.782348, best loss: 0.035105, accuracy: 19.08%\n",
      "35 loss:  1.6206597\n",
      "35 Validation accuracy: 0.20914777\n",
      "epoch: 35, validation loss: 1.620660, best loss: 0.035105, accuracy: 20.91%\n",
      "36 loss:  1.6470222\n",
      "36 Validation accuracy: 0.22009382\n",
      "epoch: 36, validation loss: 1.647022, best loss: 0.035105, accuracy: 22.01%\n",
      "37 loss:  1.6143546\n",
      "37 Validation accuracy: 0.22009382\n",
      "epoch: 37, validation loss: 1.614355, best loss: 0.035105, accuracy: 22.01%\n",
      "38 loss:  1.6475213\n",
      "38 Validation accuracy: 0.22009382\n",
      "epoch: 38, validation loss: 1.647521, best loss: 0.035105, accuracy: 22.01%\n",
      "39 loss:  1.722914\n",
      "39 Validation accuracy: 0.18725567\n",
      "Early stopping! \n",
      "INFO:tensorflow:Restoring parameters from ./chpt11/model/ex8_1.ckpt\n",
      "Final test accuracy: 99.16%\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 200\n",
    "n_epochs_before_stop = 20\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "best_loss = np.infty\n",
    "\n",
    "# 定义一个从训练集中随机挑选50个实例的方法\n",
    "def batch_generator(X, y, size):\n",
    "    rnd_idx = np.random.permutation(len(X)) # len(X)是矩阵X的第0维的长度, 生成0..len(X)的随机数\n",
    "    n_batches = len(X) // batch_size # //是整除, mod\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch # yield定义了生成器generator, 可以用next(generator)来调用, 也可以用for循环调用\n",
    "        \n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    n_epochs_after_best = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in batch_generator(X_train_04, y_train_04, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        \n",
    "        the_loss, acc_valid = sess.run([loss, accuracy], feed_dict={X: X_valid_04, y: y_valid_04})\n",
    "        print(epoch, \"loss: \", the_loss)\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)\n",
    "        \n",
    "        if the_loss < best_loss:\n",
    "            best_loss = the_loss\n",
    "            n_epochs_after_best = 0\n",
    "            save_path = saver.save(sess, \"./chpt11/model/ex8_1.ckpt\")\n",
    "        else:\n",
    "            n_epochs_after_best += 1\n",
    "            if n_epochs_after_best > n_epochs_before_stop:\n",
    "                print(\"Early stopping! \")\n",
    "                break\n",
    "        print(\"epoch: {}, validation loss: {:.6f}, best loss: {:.6f}, accuracy: {:.2f}%\".format(\n",
    "            epoch, the_loss, best_loss, acc_valid * 100))\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./chpt11/model/ex8_1.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test_04, y: y_test_04})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev-gpu",
   "language": "python",
   "name": "dev-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
