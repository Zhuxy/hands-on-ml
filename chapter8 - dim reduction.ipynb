{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是PCA里的可解释方差\n",
    "\n",
    "参考 https://ro-che.info/articles/2017-12-11-pca-explained-variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备一组数据, 横列为样本数, 纵列为变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.42298398,  1.7352837 ,  0.411915  ],\n",
       "       [-1.54987816, -0.2647112 , -0.6524724 ],\n",
       "       [-0.06442932,  2.0994707 ,  0.7603068 ],\n",
       "       [ 0.27088135,  0.8633512 ,  0.1551048 ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[-0.42298398,  1.7352837,  0.4119150],\n",
    "[-1.54987816, -0.2647112, -0.6524724],\n",
    "[-0.06442932,  2.0994707,  0.7603068],\n",
    "[ 0.27088135,  0.8633512,  0.1551048]])\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每个变量的样本偏差:  [0.62617148 1.10689586 0.36122036]\n",
      "样本总方差:  2.0942876968141357\n",
      "变量的可解释方差:  [0.29899019 0.52853095 0.17247886]\n"
     ]
    }
   ],
   "source": [
    "# 在0轴纵列上计算每个变量的方差\n",
    "\n",
    "sample_vars = np.var(a, axis=0, ddof=1) # numpy里的var默认是均方差, 除以 N - ddof(=0), 方差是除以N-1, 所以设ddof为1\n",
    "\n",
    "print(\"每个变量的样本偏差: \", sample_vars)\n",
    "\n",
    "# 样本方差之和是total variance, 总方差\n",
    "total_var = np.sum(sample_vars)\n",
    "print(\"样本总方差: \", total_var)\n",
    "\n",
    "# 每个样本方差除以总方差既为每个变量的可解释方差\n",
    "explained_vars = sample_vars/total_var\n",
    "print(\"变量的可解释方差: \", explained_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试下sklearn里的PCA计算结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方差解释率:  [0.88235567 0.11674697 0.00089736]\n",
      "转换后的样本变量方差:  [1.84790662 0.24450174 0.00187933]\n",
      "转换后的总方差:  2.0942876968141375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA() # 不降维的情况下计算PCA\n",
    "\n",
    "b = pca.fit_transform(a)\n",
    "print(\"方差解释率: \", pca.explained_variance_ratio_) # 前2个主成分占了99%的总方差\n",
    "\n",
    "\n",
    "b_vars = np.var(b, axis=0, ddof=1)\n",
    "print(\"转换后的样本变量方差: \", b_vars)\n",
    "\n",
    "b_vars = np.sum(b_vars) # 转换后的总方差和原来一致\n",
    "print(\"转换后的总方差: \", b_vars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核PCA Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:  (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "# 准备瑞士卷数据集\n",
    "from sklearn.datasets import make_swiss_roll\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)\n",
    "y = t > 6.9\n",
    "print(\"X shape: \", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机网格搜索寻找逻辑回归问题下的最佳核PCA超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"kpca\", KernelPCA(n_components=2)),\n",
    "    (\"log_req\", LogisticRegression(solver=\"lbfgs\"))\n",
    "])\n",
    "\n",
    "param_grid = [{\n",
    "    \"kpca__kernel\": [\"rbf\", \"sigmoid\"], \n",
    "    # kpca__kernel 里的kpca对应pipeline里的kpca,意思是找pipeline里名字为kpca的transformer的kernel参数\n",
    "    \"kpca__gamma\": np.linspace(0.03, 0.05, 10)\n",
    "}]\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>使用交叉验证的方格搜索来寻找可以最小化重建前图像误差的核方法和超参数</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLE局部线性嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.manifold import LocallyLinearEmbedding\n",
    "\n",
    "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)\n",
    "\n",
    "X_reduced = lle.fit_transform(X)\n",
    "\n",
    "X_reduced.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
